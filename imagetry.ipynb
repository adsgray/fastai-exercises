{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and testing set\n",
    "# define shuffle data loader?\n",
    "# got csv files from https://pjreddie.com/projects/mnist-in-csv/\n",
    "HOME=\"/home/adsgray\"\n",
    "DIR=HOME + \"/code/mnist\"\n",
    "TRAINING=DIR + \"/mnist_train.csv\"\n",
    "TESTING=DIR + \"/mnist_test.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingfile = open(TRAINING, \"r\")\n",
    "testingfile = open(TESTING, \"r\")\n",
    "traininglines = trainingfile.readlines()\n",
    "testinglines = testingfile.readlines()\n",
    "\n",
    "def tensor_from_string(s):\n",
    "    l = [int(i) for i in s.split(\",\")]\n",
    "    #map(int, l)\n",
    "    return tensor(l)\n",
    "\n",
    "    \n",
    "training_raw = torch.stack([tensor_from_string(s) for s in traininglines])\n",
    "testing_raw = torch.stack([tensor_from_string(s) for s in testinglines])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_labels = training_raw[:,0]\n",
    "training_images = training_raw[:,1:].float()/255\n",
    "\n",
    "testing_labels = testing_raw[:,0]\n",
    "testing_images = testing_raw[:,1:].float()/255\n",
    "#show_image(t[0].view(28,28))\n",
    "#testing_labels[59]\n",
    "#show_image(testing_images[59].view(28,28))\n",
    "\n",
    "testing_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to select a random sample\n",
    "import random\n",
    "\n",
    "def random_indexes(num, max):\n",
    "    return [random.randrange(0,max) for i in range(num)]\n",
    "\n",
    "def training_sample(num):\n",
    "    max = training_images.shape[0]\n",
    "    idxs = random_indexes(num, max)\n",
    "    return [(training_images[idxs], training_labels[idxs])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function\n",
    "\n",
    "from torch import exp\n",
    "def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n",
    "def sigmoid(x): return 1/(1+torch.exp(-x))\n",
    "\n",
    "fudge=1.\n",
    "\n",
    "def mnist_loss(predictions, targets):\n",
    "    return F.nll_loss(F.log_softmax(predictions, dim=1), targets)\n",
    "\n",
    "    \n",
    "def mnist_loss2(predictions, targets):\n",
    "    lp = torch.log(predictions)\n",
    "    sm = softmax(lp)\n",
    "    #print(\"sm: \", sm)\n",
    "    predictions = -sm\n",
    "    #predictions = softmax(predictions)\n",
    "    return predictions[range(targets.shape[0]), targets].mean()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "\n",
    "width=400\n",
    "w1 = init_params((28*28,width))\n",
    "b1 = init_params(width)\n",
    "w2 = init_params((width,10))\n",
    "b2 = init_params(10)\n",
    "\n",
    "params = (w1,b1,w2,b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define simple network\n",
    "def simple_net(xb): \n",
    "    res = xb@w1 + b1\n",
    "    # relu\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_acc = True\n",
    "\n",
    "def accuracy(preds, y):\n",
    "    prednums = preds[range(y.shape[0]),y]\n",
    "    maxes = torch.max(preds, dim=1)\n",
    "    cor = prednums == maxes.values\n",
    "    acc = cor.float().mean()\n",
    "    return acc\n",
    "    \n",
    "\n",
    "\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    acc = accuracy(preds, yb)\n",
    "    if print_acc: \n",
    "        print(acc)\n",
    "    #print(\"preds: \", preds)\n",
    "    #print(\"yb: \", yb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    print(\"loss: \", loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "def train_epoch(size, model, lr, params):\n",
    "    dl = training_sample(size)\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            #print(\"grad: \", p.grad)\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds:  tensor([[-57.3986,   5.6491,  49.0951,  10.9226, -25.4065,  29.9682, 163.8712, -65.1435,  52.6510, -57.2946]], grad_fn=<AddBackward0>)\n",
      "tensor([6])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = training_sample(1)\n",
    "\n",
    "for (d,y) in t:\n",
    "    p = simple_net(d)\n",
    "    #print(d.shape)\n",
    "    show_image(d.view(28,28))\n",
    "    print(\"preds: \", p)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9137)\n",
      "loss:  tensor(2.8806, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9110)\n",
      "loss:  tensor(2.9681, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9213)\n",
      "loss:  tensor(2.7309, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9140)\n",
      "loss:  tensor(3.3449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9153)\n",
      "loss:  tensor(2.9271, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9160)\n",
      "loss:  tensor(2.5050, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9137)\n",
      "loss:  tensor(2.7734, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9110)\n",
      "loss:  tensor(2.9910, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9150)\n",
      "loss:  tensor(2.9301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9127)\n",
      "loss:  tensor(2.7962, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9140)\n",
      "loss:  tensor(2.7533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9153)\n",
      "loss:  tensor(2.6748, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9150)\n",
      "loss:  tensor(2.7554, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9097)\n",
      "loss:  tensor(2.9650, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9207)\n",
      "loss:  tensor(2.8222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9120)\n",
      "loss:  tensor(3.1354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9100)\n",
      "loss:  tensor(2.8646, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9193)\n",
      "loss:  tensor(2.3536, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9170)\n",
      "loss:  tensor(2.8482, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9090)\n",
      "loss:  tensor(2.8725, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "#https://stackoverflow.com/questions/60032073/select-specific-rows-of-2d-pytorch-tensor\n",
    "\n",
    "\n",
    "#def mse(preds, targets): return ((preds-targets)**2).mean().sqrt()\n",
    "#def linear1(xb): return xb@weights + bias\n",
    "\n",
    "\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "\n",
    "lr = 0.08\n",
    "num=20\n",
    "size=3000\n",
    "# train\n",
    "def train_model(size, model, params, lr, num):\n",
    "    for i in range(num):\n",
    "        train_epoch(size, model, lr, params)\n",
    "       \n",
    "\n",
    "\n",
    "#print(params)\n",
    "train_model(size, simple_net, params, lr, num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out model to disk\n",
    "import pickle\n",
    "\n",
    "def save_model():\n",
    "    model = {'w1':w1, 'b1': b1, 'w2':w2, 'b2': b2}\n",
    "    with open('mymodel.pkl', 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_model():\n",
    "    with open('filename.pickle', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    w1 = model['w1']\n",
    "    b1 = model['b1']\n",
    "    w2 = model['w2']\n",
    "    b2 = model['b2']\n",
    "\n",
    "\n",
    "    \n",
    "save_model()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
