{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and testing set\n",
    "# define shuffle data loader?\n",
    "# got csv files from https://pjreddie.com/projects/mnist-in-csv/\n",
    "HOME=\"/home/adsgray\"\n",
    "DIR=HOME + \"/code/mnist\"\n",
    "TRAINING=DIR + \"/mnist_train.csv\"\n",
    "TESTING=DIR + \"/mnist_test.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingfile = open(TRAINING, \"r\")\n",
    "testingfile = open(TESTING, \"r\")\n",
    "traininglines = trainingfile.readlines()\n",
    "testinglines = testingfile.readlines()\n",
    "\n",
    "def tensor_from_string(s):\n",
    "    l = [int(i) for i in s.split(\",\")]\n",
    "    #map(int, l)\n",
    "    return tensor(l)\n",
    "\n",
    "    \n",
    "training_raw = torch.stack([tensor_from_string(s) for s in traininglines])\n",
    "testing_raw = torch.stack([tensor_from_string(s) for s in testinglines])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_labels = training_raw[:,0]\n",
    "training_images = training_raw[:,1:].float()/255\n",
    "\n",
    "testing_labels = testing_raw[:,0]\n",
    "testing_images = testing_raw[:,1:].float()/255\n",
    "#show_image(t[0].view(28,28))\n",
    "#testing_labels[59]\n",
    "#show_image(testing_images[59].view(28,28))\n",
    "\n",
    "testing_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to select a random sample\n",
    "import random\n",
    "\n",
    "class BatchSampler:   \n",
    "        \n",
    "    def __init__(self, source, source_labels):\n",
    "        self.source = source\n",
    "        self.source_labels = source_labels\n",
    "        self.max = source.shape[0]\n",
    "        self.reset_indexes_avail()\n",
    "        \n",
    "    def get_source_size(self):\n",
    "        return self.source.shape[0]\n",
    "        \n",
    "    def reset_indexes_avail(self):\n",
    "        self.indexes_avail = set(range(0, self.max))\n",
    "    \n",
    "    def random_indexes_from_set(self, num):\n",
    "        if num > len(self.indexes_avail):\n",
    "            num = len(self.indexes_avail)\n",
    "        ret = random.sample(self.indexes_avail, num)\n",
    "        self.indexes_avail.difference_update(ret)\n",
    "        return ret\n",
    "        \n",
    "    def get_sample(self, num):\n",
    "        idxs = list(self.random_indexes_from_set(num))\n",
    "        return [(self.source[idxs], self.source_labels[idxs])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "b = BatchSampler(training_images, training_labels)\n",
    "samp = b.get_sample(10)\n",
    "#print(samp)\n",
    "x,y = samp\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function\n",
    "\n",
    "from torch import exp\n",
    "def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n",
    "def sigmoid(x): return 1/(1+torch.exp(-x))\n",
    "\n",
    "fudge=1.\n",
    "\n",
    "def mnist_loss(predictions, targets):\n",
    "    return F.nll_loss(F.log_softmax(predictions, dim=1), targets)\n",
    "\n",
    "    \n",
    "def mnist_loss2(predictions, targets):\n",
    "    lp = torch.log(predictions)\n",
    "    sm = softmax(lp)\n",
    "    #print(\"sm: \", sm)\n",
    "    predictions = -sm\n",
    "    #predictions = softmax(predictions)\n",
    "    return predictions[range(targets.shape[0]), targets].mean()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "\n",
    "width=256\n",
    "w1 = init_params((28*28,width))\n",
    "b1 = init_params(width)\n",
    "w2 = init_params((width,10))\n",
    "b2 = init_params(10)\n",
    "\n",
    "params = (w1,b1,w2,b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define simple network\n",
    "def simple_net(xb): \n",
    "    res = xb@w1 + b1\n",
    "    # relu\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect=list()\n",
    "badguesses=list()\n",
    "correct=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_acc = True\n",
    "save_incor = False\n",
    "\n",
    "\n",
    "def accuracy(preds, x, y):\n",
    "    prednums = preds[range(y.shape[0]),y]\n",
    "    maxes = torch.max(preds, dim=1)\n",
    "    cor = prednums == maxes.values\n",
    "    \n",
    "    if save_incor:\n",
    "        incor = [i for i,j in zip(x,cor) if not j]\n",
    "        wrongguess = [g for g,j in zip(preds,cor) if not j]\n",
    "        c = [y for y,j in zip(y,cor) if not j]\n",
    "        incorrect.extend(incor)\n",
    "        badguesses.extend(wrongguess)\n",
    "        correct.extend(c)\n",
    "    \n",
    "    acc = cor.float().mean()\n",
    "    return acc\n",
    "    \n",
    "\n",
    "\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    if print_acc:\n",
    "        acc = accuracy(preds, xb, yb)\n",
    "        print(acc)\n",
    "    #print(\"preds: \", preds)\n",
    "    #print(\"yb: \", yb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    print(\"loss: \", loss)\n",
    "    loss.backward()\n",
    "\n",
    "# weight decay\n",
    "wd = .01\n",
    "def train_epoch_internal(model, lr, params, dl):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            #print(\"grad: \", p.grad)\n",
    "            #parameters.grad += wd * 2 * parameters\n",
    "            p.grad += wd * p.data\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "            \n",
    "    \n",
    "def train_epoch(b, size, model, lr, params):\n",
    "    dl = b.get_sample(size)\n",
    "    train_epoch_internal(model, lr, params, dl)\n",
    "\n",
    "def train_on_incorrect(model, lr, params):\n",
    "    dl = [(torch.stack(incorrect), torch.stack(correct))]\n",
    "    train_epoch_internal(model, lr, params, dl)\n",
    "\n",
    "# run the model against the test set and print accuracy\n",
    "def test_model(xb, yb, model, params):\n",
    "    preds = model(xb)\n",
    "    acc = accuracy(preds,xb,yb)\n",
    "    print(\"test accuracy: \", acc)\n",
    "    # reset params gradients so this test doesn't affect training?\n",
    "    for p in params:\n",
    "        p.grad.zero_()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 2, 4, 3, 3, 1, 7, 9, 5, 5, 6, 7, 3, 5, 8, 5, 5, 7, 8, 3, 6, 9, 7, 9, 0, 4, 8, 3, 8, 3, 2, 3, 7, 2, 2, 3, 9, 2, 8, 3, 2, 4, 3, 8, 8, 2, 9, 3, 5, 3, 8, 7, 4, 6, 5, 0, 8, 6, 8, 2, 3, 8, 4, 7,\n",
       "        7, 8, 1, 5, 2, 5, 8, 7, 0, 8, 8, 7, 2, 5, 4, 8, 4, 9, 7, 8, 7, 8, 5, 5, 2, 3, 8, 2, 0, 3, 5, 3, 8, 4, 9, 3, 1, 3, 8, 5, 1, 8, 8, 2, 2, 5, 2, 9, 2, 0, 5, 2, 5, 8, 3, 8, 5, 8, 3, 3, 3, 6, 7, 0,\n",
       "        3, 4, 7, 4, 2, 6, 6, 5, 4, 6, 2, 2, 3, 9, 2, 4, 6, 7, 5, 9, 7, 7, 5, 5, 5, 3, 2, 9, 8, 0, 2, 7, 9, 5, 3, 3, 5, 7, 3, 3, 7, 3, 2, 7, 3, 0, 6, 7, 9, 9, 7, 9, 1, 6, 9, 6, 9, 9, 5, 7, 3, 0, 1, 8,\n",
       "        9, 8, 9, 6, 9, 0, 5, 4, 8, 8, 6, 8, 6, 6, 3, 7, 4, 2, 9, 7, 3, 6, 1, 5, 5, 3, 9, 8, 6, 3, 4, 7, 5, 9, 5, 7, 3, 3, 9, 5, 4, 8, 8, 4, 9, 2, 7, 1, 3, 7, 6, 3, 3, 3, 0, 7, 5, 3, 7, 6, 6])"
      ]
     },
     "execution_count": 1029,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-f954bbafbbac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadguesses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guess: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadguesses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"actual: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "n=7\n",
    "show_image(incorrect[n].view(28,28))\n",
    "print(badguesses[n])\n",
    "print(\"guess: \", torch.argmax(badguesses[n]))\n",
    "print(\"actual: \", correct[n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds:  tensor([[-33.7046, -28.9055,   1.0306,  27.8328, -25.2414,  22.7546,  40.7507, -12.3116,  52.7750,  52.4598]], grad_fn=<AddBackward0>)\n",
      "tensor([3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = training_sample(1)\n",
    "\n",
    "for (d,y) in t:\n",
    "    p = simple_net(d)\n",
    "    #print(d.shape)\n",
    "    show_image(d.view(28,28))\n",
    "    print(\"preds: \", p)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks:  200\n",
      "tensor(0.8167)\n",
      "loss:  tensor(0.9004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.6959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7364, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.6395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.7878, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.9006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7225, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6905, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7160, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.9366, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.7524, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.5511, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8200)\n",
      "loss:  tensor(0.8718, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8200)\n",
      "loss:  tensor(0.9743, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(0.4632, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.8404, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.5778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.6268, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.7626, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.6057, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.5278, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8733)\n",
      "loss:  tensor(0.6431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.6425, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.7839, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8700)\n",
      "loss:  tensor(0.4243, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8167)\n",
      "loss:  tensor(0.7278, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8433)\n",
      "loss:  tensor(0.8578, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(0.6348, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8767)\n",
      "loss:  tensor(0.5424, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.9263, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.7058, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.6829, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8733)\n",
      "loss:  tensor(0.7481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8433)\n",
      "loss:  tensor(0.7485, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8133)\n",
      "loss:  tensor(0.8588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.7550, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.7713, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(0.6561, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(0.7431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.7219, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8967)\n",
      "loss:  tensor(0.4706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.8679, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.8725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.7208, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8667)\n",
      "loss:  tensor(0.5168, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8267)\n",
      "loss:  tensor(0.7583, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8167)\n",
      "loss:  tensor(0.9083, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.5749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.7882, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.7421, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.7900, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.7217, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8667)\n",
      "loss:  tensor(0.4807, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.7112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.6696, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.6585, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.7630, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8267)\n",
      "loss:  tensor(0.7101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8667)\n",
      "loss:  tensor(0.7308, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.8325, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6740, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8100)\n",
      "loss:  tensor(0.9400, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.7978, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8267)\n",
      "loss:  tensor(0.8239, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.7778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6723, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.5820, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.7569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.5750, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.7111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8167)\n",
      "loss:  tensor(0.7374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.7055, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6875, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.5710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.7120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.6409, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.6989, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8433)\n",
      "loss:  tensor(0.7466, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.8201, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.4911, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.5831, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7597, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.4883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.5067, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.5667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.8240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.6036, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8667)\n",
      "loss:  tensor(0.6518, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8700)\n",
      "loss:  tensor(0.7025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.8588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7921, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.5037, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8667)\n",
      "loss:  tensor(0.6184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.7257, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6835, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6789, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6857, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.7700, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.5957, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8667)\n",
      "loss:  tensor(0.6258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.5217, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.6396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8433)\n",
      "loss:  tensor(0.7710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.7754, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.8287, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8700)\n",
      "loss:  tensor(0.6567, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.5252, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8433)\n",
      "loss:  tensor(0.6438, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8133)\n",
      "loss:  tensor(0.7790, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.6191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.7671, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7967)\n",
      "loss:  tensor(0.8235, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.9422, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.7363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.6472, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.6612, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8000)\n",
      "loss:  tensor(0.9573, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8167)\n",
      "loss:  tensor(0.9743, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.6695, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.7232, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.5973, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.6782, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.7235, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.8842, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8267)\n",
      "loss:  tensor(0.9082, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.7053, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.6764, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7730, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.7377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8767)\n",
      "loss:  tensor(0.5816, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8700)\n",
      "loss:  tensor(0.6638, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.7548, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8433)\n",
      "loss:  tensor(0.6420, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.7513, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8267)\n",
      "loss:  tensor(0.7344, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.7052, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(0.3648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8067)\n",
      "loss:  tensor(0.9817, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8100)\n",
      "loss:  tensor(0.8973, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.6404, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.7650, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.6674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8733)\n",
      "loss:  tensor(0.6683, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8100)\n",
      "loss:  tensor(0.7724, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(0.5823, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.5965, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.6440, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.5919, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.5730, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9000)\n",
      "loss:  tensor(0.4321, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8167)\n",
      "loss:  tensor(0.8307, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8867)\n",
      "loss:  tensor(0.5600, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8667)\n",
      "loss:  tensor(0.5549, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8100)\n",
      "loss:  tensor(0.7378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7065, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.7735, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8733)\n",
      "loss:  tensor(0.5837, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.9354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.9169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.6618, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8400)\n",
      "loss:  tensor(0.7569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6545, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.4904, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7919, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.7823, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8267)\n",
      "loss:  tensor(0.5781, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.6340, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.9303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.8222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(0.5011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8667)\n",
      "loss:  tensor(0.6267, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8300)\n",
      "loss:  tensor(0.7016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.6950, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.5367, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8333)\n",
      "loss:  tensor(0.8034, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.7449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8467)\n",
      "loss:  tensor(0.6383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.4969, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.5240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.7280, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(0.4894, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(0.5245, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.7093, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.9742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8633)\n",
      "loss:  tensor(0.6698, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(1.0219, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8500)\n",
      "loss:  tensor(0.6656, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8200)\n",
      "loss:  tensor(0.7309, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.8172, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8567)\n",
      "loss:  tensor(0.5723, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8233)\n",
      "loss:  tensor(0.7621, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.9023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8533)\n",
      "loss:  tensor(0.7337, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8200)\n",
      "loss:  tensor(0.7119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8367)\n",
      "loss:  tensor(0.7949, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.6486, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8600)\n",
      "loss:  tensor(0.6284, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "#https://stackoverflow.com/questions/60032073/select-specific-rows-of-2d-pytorch-tensor\n",
    "\n",
    "\n",
    "#def mse(preds, targets): return ((preds-targets)**2).mean().sqrt()\n",
    "#def linear1(xb): return xb@weights + bias\n",
    "\n",
    "\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "\n",
    "b = BatchSampler(training_images, training_labels)\n",
    "lr = 0.00001\n",
    "num=20\n",
    "size=300\n",
    "chunks = int(b.get_source_size() / size)\n",
    "print(\"chunks: \", chunks)\n",
    "# train\n",
    "\n",
    "def train_model(b, size, model, params, lr, num):\n",
    "    for i in range(chunks):\n",
    "        train_epoch(b, size, model, lr, params)\n",
    "       \n",
    "\n",
    "\n",
    "#print(params)\n",
    "\n",
    "train_model(b, size, simple_net, params, lr, num)\n",
    "#test_model(testing_images, testing_labels, simple_net, params)\n",
    "#save_model()\n",
    "\n",
    "#train_on_incorrect(simple_net, lr, params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  tensor(0.8497)\n"
     ]
    }
   ],
   "source": [
    "test_model(testing_images, testing_labels, simple_net, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out model to disk\n",
    "import pickle\n",
    "\n",
    "def save_model():\n",
    "    model = {'w1':w1, 'b1': b1, 'w2':w2, 'b2': b2}\n",
    "    with open('mymodel.pkl', 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_model():\n",
    "    with open('mymodel.pkl', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    w1 = model['w1']\n",
    "    b1 = model['b1']\n",
    "    w2 = model['w2']\n",
    "    b2 = model['b2']\n",
    "\n",
    "\n",
    "    \n",
    "#save_model()\n",
    "read_model()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
