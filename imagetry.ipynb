{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and testing set\n",
    "# define shuffle data loader?\n",
    "# got csv files from https://pjreddie.com/projects/mnist-in-csv/\n",
    "HOME=\"/home/adsgray\"\n",
    "DIR=HOME + \"/code/mnist\"\n",
    "TRAINING=DIR + \"/mnist_train.csv\"\n",
    "TESTING=DIR + \"/mnist_test.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingfile = open(TRAINING, \"r\")\n",
    "testingfile = open(TESTING, \"r\")\n",
    "traininglines = trainingfile.readlines()\n",
    "testinglines = testingfile.readlines()\n",
    "\n",
    "def tensor_from_string(s):\n",
    "    l = [int(i) for i in s.split(\",\")]\n",
    "    #map(int, l)\n",
    "    return tensor(l)\n",
    "\n",
    "    \n",
    "training_raw = torch.stack([tensor_from_string(s) for s in traininglines])\n",
    "testing_raw = torch.stack([tensor_from_string(s) for s in testinglines])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_labels = training_raw[:,0]\n",
    "training_images = training_raw[:,1:].float()/255\n",
    "\n",
    "testing_labels = testing_raw[:,0]\n",
    "testing_images = testing_raw[:,1:].float()/255\n",
    "#show_image(t[0].view(28,28))\n",
    "#testing_labels[59]\n",
    "#show_image(testing_images[59].view(28,28))\n",
    "\n",
    "testing_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to select a random sample\n",
    "import random\n",
    "\n",
    "training_max = training_images.shape[0]\n",
    "training_indexes_available = set(range(0,training_max))\n",
    "\n",
    "def reset_training_indexes():\n",
    "    training_indexes_available = set(range(0,training_max))\n",
    "\n",
    "def random_indexes_from_set(num):\n",
    "    ret = random.sample(training_indexes_available, num)\n",
    "    training_indexes_available.difference_update(ret)\n",
    "    return ret\n",
    "\n",
    "def random_indexes(num, max):\n",
    "    return [random.randrange(0,max) for i in range(num)]\n",
    "\n",
    "def training_sample(num):\n",
    "    max = training_images.shape[0]\n",
    "    #idxs = random_indexes(num, training_max)\n",
    "    idxs = list(random_indexes_from_set(num))\n",
    "    return [(training_images[idxs], training_labels[idxs])]\n",
    "\n",
    "#print(training_indexes_available)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function\n",
    "\n",
    "from torch import exp\n",
    "def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n",
    "def sigmoid(x): return 1/(1+torch.exp(-x))\n",
    "\n",
    "fudge=1.\n",
    "\n",
    "def mnist_loss(predictions, targets):\n",
    "    return F.nll_loss(F.log_softmax(predictions, dim=1), targets)\n",
    "\n",
    "    \n",
    "def mnist_loss2(predictions, targets):\n",
    "    lp = torch.log(predictions)\n",
    "    sm = softmax(lp)\n",
    "    #print(\"sm: \", sm)\n",
    "    predictions = -sm\n",
    "    #predictions = softmax(predictions)\n",
    "    return predictions[range(targets.shape[0]), targets].mean()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "\n",
    "width=256\n",
    "w1 = init_params((28*28,width))\n",
    "b1 = init_params(width)\n",
    "w2 = init_params((width,10))\n",
    "b2 = init_params(10)\n",
    "\n",
    "params = (w1,b1,w2,b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define simple network\n",
    "def simple_net(xb): \n",
    "    res = xb@w1 + b1\n",
    "    # relu\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect=list()\n",
    "badguesses=list()\n",
    "correct=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_acc = True\n",
    "save_incor = True\n",
    "\n",
    "\n",
    "def accuracy(preds, x, y):\n",
    "    prednums = preds[range(y.shape[0]),y]\n",
    "    maxes = torch.max(preds, dim=1)\n",
    "    cor = prednums == maxes.values\n",
    "    \n",
    "    if save_incor:\n",
    "        incor = [i for i,j in zip(x,cor) if not j]\n",
    "        wrongguess = [g for g,j in zip(preds,cor) if not j]\n",
    "        c = [y for y,j in zip(y,cor) if not j]\n",
    "        incorrect.extend(incor)\n",
    "        badguesses.extend(wrongguess)\n",
    "        correct.extend(c)\n",
    "    \n",
    "    acc = cor.float().mean()\n",
    "    return acc\n",
    "    \n",
    "\n",
    "\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    if print_acc:\n",
    "        acc = accuracy(preds, xb, yb)\n",
    "        print(acc)\n",
    "    #print(\"preds: \", preds)\n",
    "    #print(\"yb: \", yb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    print(\"loss: \", loss)\n",
    "    loss.backward()\n",
    "\n",
    "# weight decay\n",
    "wd = .005\n",
    "def train_epoch_internal(model, lr, params, dl):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            #print(\"grad: \", p.grad)\n",
    "            #parameters.grad += wd * 2 * parameters\n",
    "            p.grad += wd * p.data\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "            \n",
    "    \n",
    "def train_epoch(size, model, lr, params):\n",
    "    dl = training_sample(size)\n",
    "    train_epoch_internal(model, lr, params, dl)\n",
    "\n",
    "def train_on_incorrect(model, lr, params):\n",
    "    dl = [(torch.stack(incorrect), torch.stack(correct))]\n",
    "    train_epoch_internal(model, lr, params, dl)\n",
    "\n",
    "# run the model against the test set and print accuracy\n",
    "def test_model(xb, yb, model, params):\n",
    "    preds = model(xb)\n",
    "    acc = accuracy(preds,xb,yb)\n",
    "    print(\"test accuracy: \", acc)\n",
    "    # reset params gradients so this test doesn't affect training?\n",
    "    for p in params:\n",
    "        p.grad.zero_()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 2, 4, 3, 3, 1, 7, 9, 5, 5, 6, 7, 3, 5, 8, 5, 5, 7, 8, 3, 6, 9, 7, 9, 0, 4, 8, 3, 8, 3, 2, 3, 7, 2, 2, 3, 9, 2, 8, 3, 2, 4, 3, 8, 8, 2, 9, 3, 5, 3, 8, 7, 4, 6, 5, 0, 8, 6, 8, 2, 3, 8, 4, 7,\n",
       "        7, 8, 1, 5, 2, 5, 8, 7, 0, 8, 8, 7, 2, 5, 4, 8, 4, 9, 7, 8, 7, 8, 5, 5, 2, 3, 8, 2, 0, 3, 5, 3, 8, 4, 9, 3, 1, 3, 8, 5, 1, 8, 8, 2, 2, 5, 2, 9, 2, 0, 5, 2, 5, 8, 3, 8, 5, 8, 3, 3, 3, 6, 7, 0,\n",
       "        3, 4, 7, 4, 2, 6, 6, 5, 4, 6, 2, 2, 3, 9, 2, 4, 6, 7, 5, 9, 7, 7, 5, 5, 5, 3, 2, 9, 8, 0, 2, 7, 9, 5, 3, 3, 5, 7, 3, 3, 7, 3, 2, 7, 3, 0, 6, 7, 9, 9, 7, 9, 1, 6, 9, 6, 9, 9, 5, 7, 3, 0, 1, 8,\n",
       "        9, 8, 9, 6, 9, 0, 5, 4, 8, 8, 6, 8, 6, 6, 3, 7, 4, 2, 9, 7, 3, 6, 1, 5, 5, 3, 9, 8, 6, 3, 4, 7, 5, 9, 5, 7, 3, 3, 9, 5, 4, 8, 8, 4, 9, 2, 7, 1, 3, 7, 6, 3, 3, 3, 0, 7, 5, 3, 7, 6, 6])"
      ]
     },
     "execution_count": 1029,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 12.8066, -33.6041, -28.2903, -19.1194,  13.8405, -36.2888, -15.7823,  58.9041,   0.1675,  34.9782], grad_fn=<UnbindBackward>)\n",
      "guess:  tensor(7)\n",
      "actual:  tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=6\n",
    "show_image(incorrect[n].view(28,28))\n",
    "print(badguesses[n])\n",
    "print(\"guess: \", torch.argmax(badguesses[n]))\n",
    "print(\"actual: \", correct[n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds:  tensor([[  74.9436,   92.1547, -326.0612,   69.1091, -101.1812,   12.6790,   73.7510,   29.3107,  -37.1635,  194.3238]], grad_fn=<AddBackward0>)\n",
      "tensor([9])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = training_sample(1)\n",
    "\n",
    "for (d,y) in t:\n",
    "    p = simple_net(d)\n",
    "    #print(d.shape)\n",
    "    show_image(d.view(28,28))\n",
    "    print(\"preds: \", p)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks:  149\n",
      "tensor(0.8850)\n",
      "loss:  tensor(3.2863, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8850)\n",
      "loss:  tensor(3.1395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9200)\n",
      "loss:  tensor(2.1946, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9000)\n",
      "loss:  tensor(1.9763, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(3.0569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9150)\n",
      "loss:  tensor(2.1802, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(2.7350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9175)\n",
      "loss:  tensor(1.8186, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8725)\n",
      "loss:  tensor(3.2158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(2.6953, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(3.1916, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8750)\n",
      "loss:  tensor(3.4329, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9175)\n",
      "loss:  tensor(1.8514, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9100)\n",
      "loss:  tensor(2.5495, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(2.9119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8925)\n",
      "loss:  tensor(2.3418, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(2.9975, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9000)\n",
      "loss:  tensor(2.8663, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8925)\n",
      "loss:  tensor(3.1478, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9150)\n",
      "loss:  tensor(2.3739, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.6831, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9200)\n",
      "loss:  tensor(1.8108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(4.6358, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8775)\n",
      "loss:  tensor(3.0869, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(3.4748, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8750)\n",
      "loss:  tensor(3.5754, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(2.9538, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8675)\n",
      "loss:  tensor(3.3888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.9638, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8750)\n",
      "loss:  tensor(4.1365, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9200)\n",
      "loss:  tensor(2.4634, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8725)\n",
      "loss:  tensor(3.1303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8975)\n",
      "loss:  tensor(3.3038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.6135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.4344, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(2.7725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(2.4230, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8875)\n",
      "loss:  tensor(3.1480, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8575)\n",
      "loss:  tensor(4.3031, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9100)\n",
      "loss:  tensor(2.5897, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(2.7782, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(3.1800, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9175)\n",
      "loss:  tensor(2.2268, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9100)\n",
      "loss:  tensor(2.6505, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(2.3146, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(3.2722, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9000)\n",
      "loss:  tensor(2.4467, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8725)\n",
      "loss:  tensor(3.5245, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(2.8760, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8825)\n",
      "loss:  tensor(2.6715, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8650)\n",
      "loss:  tensor(3.0467, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9225)\n",
      "loss:  tensor(2.2808, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8975)\n",
      "loss:  tensor(2.9029, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(3.1377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9150)\n",
      "loss:  tensor(2.9082, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9200)\n",
      "loss:  tensor(3.0728, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(3.1764, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8775)\n",
      "loss:  tensor(2.9983, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8875)\n",
      "loss:  tensor(3.4574, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8875)\n",
      "loss:  tensor(3.1776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9125)\n",
      "loss:  tensor(2.5118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(2.0024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(3.5480, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(2.4336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8850)\n",
      "loss:  tensor(3.5836, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9000)\n",
      "loss:  tensor(2.2523, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(2.6437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(3.5254, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(2.7561, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9100)\n",
      "loss:  tensor(1.8892, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8575)\n",
      "loss:  tensor(3.1373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(2.3597, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8975)\n",
      "loss:  tensor(2.7441, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9150)\n",
      "loss:  tensor(2.6584, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8750)\n",
      "loss:  tensor(3.3937, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8975)\n",
      "loss:  tensor(2.1005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(3.0201, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(2.3619, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8875)\n",
      "loss:  tensor(3.3894, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9200)\n",
      "loss:  tensor(2.5466, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9000)\n",
      "loss:  tensor(2.7409, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.9441, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9000)\n",
      "loss:  tensor(2.0056, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(2.2667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8850)\n",
      "loss:  tensor(3.1116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8975)\n",
      "loss:  tensor(2.7893, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9100)\n",
      "loss:  tensor(2.1061, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(1.7534, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(3.7004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8925)\n",
      "loss:  tensor(3.1042, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9225)\n",
      "loss:  tensor(2.3424, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8975)\n",
      "loss:  tensor(2.6158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8675)\n",
      "loss:  tensor(3.3251, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8625)\n",
      "loss:  tensor(4.6012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(2.2138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(3.0234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8875)\n",
      "loss:  tensor(2.8156, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9200)\n",
      "loss:  tensor(2.1060, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8825)\n",
      "loss:  tensor(3.0970, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.7350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(3.1593, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8850)\n",
      "loss:  tensor(2.7603, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9000)\n",
      "loss:  tensor(2.6466, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(2.6275, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(2.6211, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(2.5259, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(2.7963, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8975)\n",
      "loss:  tensor(2.5689, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8900)\n",
      "loss:  tensor(2.9538, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(2.8774, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8675)\n",
      "loss:  tensor(3.3546, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8875)\n",
      "loss:  tensor(3.2787, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.1968, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8750)\n",
      "loss:  tensor(3.7065, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.1141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(2.6220, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8650)\n",
      "loss:  tensor(4.2588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8950)\n",
      "loss:  tensor(2.4672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8875)\n",
      "loss:  tensor(2.7714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(3.0278, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8725)\n",
      "loss:  tensor(3.3400, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8775)\n",
      "loss:  tensor(2.7330, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(2.7378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8825)\n",
      "loss:  tensor(2.8676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(2.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8700)\n",
      "loss:  tensor(2.9632, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(2.7424, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8775)\n",
      "loss:  tensor(3.0184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8775)\n",
      "loss:  tensor(2.9590, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9125)\n",
      "loss:  tensor(3.0835, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(3.1461, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8725)\n",
      "loss:  tensor(2.7172, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(3.2685, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8850)\n",
      "loss:  tensor(2.8954, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8800)\n",
      "loss:  tensor(2.5490, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8775)\n",
      "loss:  tensor(3.5419, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8825)\n",
      "loss:  tensor(2.6431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(2.2238, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8700)\n",
      "loss:  tensor(3.5125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8775)\n",
      "loss:  tensor(3.3316, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9100)\n",
      "loss:  tensor(1.9932, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9050)\n",
      "loss:  tensor(3.0647, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9275)\n",
      "loss:  tensor(1.2045, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025)\n",
      "loss:  tensor(2.7749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9125)\n",
      "loss:  tensor(2.6512, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8875)\n",
      "loss:  tensor(3.0729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9175)\n",
      "loss:  tensor(1.8845, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8675)\n",
      "loss:  tensor(4.3743, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075)\n",
      "loss:  tensor(3.3288, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "#https://stackoverflow.com/questions/60032073/select-specific-rows-of-2d-pytorch-tensor\n",
    "\n",
    "\n",
    "#def mse(preds, targets): return ((preds-targets)**2).mean().sqrt()\n",
    "#def linear1(xb): return xb@weights + bias\n",
    "\n",
    "\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "\n",
    "lr = 0.005\n",
    "num=20\n",
    "size=400\n",
    "chunks = int(training_max / size) - 1\n",
    "print(\"chunks: \", chunks)\n",
    "# train\n",
    "\n",
    "def train_model(size, model, params, lr, num):\n",
    "    for i in range(chunks):\n",
    "        train_epoch(size, model, lr, params)\n",
    "       \n",
    "\n",
    "\n",
    "#print(params)\n",
    "reset_training_indexes()\n",
    "train_model(size, simple_net, params, lr, num)\n",
    "#test_model(testing_images, testing_labels, simple_net, params)\n",
    "#save_model()\n",
    "\n",
    "#train_on_incorrect(simple_net, lr, params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  tensor(0.8935)\n"
     ]
    }
   ],
   "source": [
    "test_model(testing_images, testing_labels, simple_net, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out model to disk\n",
    "import pickle\n",
    "\n",
    "def save_model():\n",
    "    model = {'w1':w1, 'b1': b1, 'w2':w2, 'b2': b2}\n",
    "    with open('mymodel.pkl', 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_model():\n",
    "    with open('mymodel.pkl', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    w1 = model['w1']\n",
    "    b1 = model['b1']\n",
    "    w2 = model['w2']\n",
    "    b2 = model['b2']\n",
    "\n",
    "\n",
    "    \n",
    "#save_model()\n",
    "read_model()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
